{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy: 1.16.2\n",
      "torch: 1.1.0\n",
      "pytorch_pretrained_bert: 0.6.2\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "from tqdm import trange\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pytorch_pretrained_bert\n",
    "from pytorch_pretrained_bert import TransfoXLTokenizer, TransfoXLModel, TransfoXLLMHeadModel\n",
    "for mod in (np, torch, pytorch_pretrained_bert):\n",
    "    print(f'{mod.__name__}: {mod.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model Transformer XL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransfoXLLMHeadModel(\n",
       "  (transformer): TransfoXLModel(\n",
       "    (word_emb): AdaptiveEmbedding(\n",
       "      (emb_layers): ModuleList(\n",
       "        (0): Embedding(20000, 1024)\n",
       "        (1): Embedding(20000, 256)\n",
       "        (2): Embedding(160000, 64)\n",
       "        (3): Embedding(67735, 16)\n",
       "      )\n",
       "      (emb_projs): ParameterList(\n",
       "          (0): Parameter containing: [torch.FloatTensor of size 1024x1024]\n",
       "          (1): Parameter containing: [torch.FloatTensor of size 1024x256]\n",
       "          (2): Parameter containing: [torch.FloatTensor of size 1024x64]\n",
       "          (3): Parameter containing: [torch.FloatTensor of size 1024x16]\n",
       "      )\n",
       "    )\n",
       "    (drop): Dropout(p=0.1)\n",
       "    (layers): ModuleList(\n",
       "      (0): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (1): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (2): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (3): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (4): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (5): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (6): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (7): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (8): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (9): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (10): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (11): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (12): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (13): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (14): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (15): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (16): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (17): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pos_emb): PositionalEmbedding()\n",
       "  )\n",
       "  (crit): ProjectedAdaptiveLogSoftmax(\n",
       "    (out_layers): ModuleList(\n",
       "      (0): Linear(in_features=1024, out_features=20000, bias=True)\n",
       "      (1): Linear(in_features=256, out_features=20000, bias=True)\n",
       "      (2): Linear(in_features=64, out_features=160000, bias=True)\n",
       "      (3): Linear(in_features=16, out_features=67735, bias=True)\n",
       "    )\n",
       "    (out_projs): ParameterList(\n",
       "        (0): Parameter containing: [torch.FloatTensor of size 1024x1024]\n",
       "        (1): Parameter containing: [torch.FloatTensor of size 1024x256]\n",
       "        (2): Parameter containing: [torch.FloatTensor of size 1024x64]\n",
       "        (3): Parameter containing: [torch.FloatTensor of size 1024x16]\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "torch.random.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name_or_path = 'transfo-xl-wt103'\n",
    "tokenizer = TransfoXLTokenizer.from_pretrained(model_name_or_path)\n",
    "model = TransfoXLLMHeadModel.from_pretrained(model_name_or_path)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dummy prediction, to check vocab size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = \"Dummy\"\n",
    "line_tokenized = tokenizer.tokenize(line)\n",
    "line_indexed = tokenizer.convert_tokens_to_ids(line_tokenized)\n",
    "tokens_tensor = torch.tensor([line_indexed])\n",
    "predictions, _ = model(tokens_tensor)\n",
    "vocab_size = predictions.shape[-1]\n",
    "assert vocab_size == 267735  # WikiText-103 vocab size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimal example\n",
    "\n",
    "## Online text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Britain \n",
      " ['the', 'Britain', 'late', '1978', '1974', 'England', '1979', '1982', '2000', '1972'] \n",
      "\n",
      "and \n",
      " ['.', 'and', ',', 'by', 'in', ':', ';', 'as', 'for', 'during'] \n",
      "\n",
      "America \n",
      " ['the', 'America', 'France', 'were', 'shipped', 'Canada', 'Italy', 'Europe', 'cars', 'in'] \n",
      "\n",
      ", \n",
      " ['.', ',', 'in', 'and', ';', 'by', '(', ':', 'as', 'during'] \n",
      "\n",
      "but \n",
      " ['and', 'but', 'including', 'the', 'with', 'although', 'which', 'as', 'such', 'in'] \n",
      "\n",
      "the \n",
      " ['were', 'the', 'cars', 'are', 'in', 'not', 'they', 'have', 'their', 'only'] \n",
      "\n",
      "first \n",
      " ['British', 'first', 'most', 'European', 'UK', '\"', 'American', '<unk>', 'design', 'US'] \n",
      "\n",
      "two \n",
      " ['cars', 'two', 'British', 'ones', 'American', 'car', 'examples', 'horses', 'European', '<unk>'] \n",
      "\n",
      "cars \n",
      " ['were', 'cars', 'are', 'of', 'in', 'types', '@-@', ',', 'vehicles', 'prototypes'] \n",
      "\n",
      "had \n",
      " ['were', 'had', 'are', 'did', 'came', ',', 'in', 'have', 'could', 'made'] \n",
      "\n",
      "to \n",
      " ['been', 'to', 'a', 'not', 'the', 'already', 'no', 'arrived', 'yet', 'an'] \n",
      "\n",
      "have \n",
      " ['be', 'have', 'come', 'go', 'make', 'enter', 'appear', 'build', 'exist', 'meet'] \n",
      "\n",
      "been \n",
      " ['a', 'been', 'the', 'an', 'engines', 'two', 'cars', 'some', 'all', 'wheels'] \n",
      "\n",
      "a \n",
      " ['built', 'a', 'the', 'completed', 'in', 'used', 'imported', 'made', 'designed', 'produced'] \n",
      "\n",
      "\" \n",
      " ['car', '\"', 'Turbo', 'British', 'prototype', '<unk>', 'single', 'two', 'vehicle', 'product'] \n",
      "\n",
      "Turbo \n",
      " ['<unk>', 'Turbo', 'car', 'one', 'two', 'second', 'big', '\"', 'first', 'B'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "line = \"Cars were invented in\"\n",
    "max_predictions = 16\n",
    "top_k = 2\n",
    "\n",
    "line_tokenized = tokenizer.tokenize(line)\n",
    "line_indexed = tokenizer.convert_tokens_to_ids(line_tokenized)\n",
    "tokens_tensor = torch.tensor([line_indexed])\n",
    "tokens_tensor = tokens_tensor.to(device)\n",
    "mems = None\n",
    "\n",
    "for i in range(max_predictions):\n",
    "    predictions, mems = model(tokens_tensor, mems=mems)\n",
    "    context_size = tokens_tensor.shape[1]\n",
    "    assert predictions.shape == (1, context_size, vocab_size)\n",
    "    topk = torch.topk(predictions[0, -1, :], 10)\n",
    "    predicted_index = topk.indices[top_k-1].item()\n",
    "    predicted_index = torch.tensor([[predicted_index]]).to(device)\n",
    "    tokens_tensor = torch.cat((tokens_tensor, predicted_index), dim=1)\n",
    "    \n",
    "    predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
    "    print(predicted_token, end=' ', flush=True)\n",
    "    print('\\n', tokenizer.convert_ids_to_tokens(topk.indices), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE**: this text is generated choosing at each step the top_k most probable token.\n",
    "> This is **online text generation**, since at each step, the model only knows the past."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Off-line text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[top 1 token] PROMPT: Cars were invented in\n",
      "the . America . and the first two were were to be been a car <unk> \n",
      "\n",
      "[top 2 token] PROMPT: Cars were invented in\n",
      "Britain and the , but a only car cars had been have a the \" Turbo \n",
      "\n",
      "[top 3 token] PROMPT: Cars were invented in\n",
      "America , American and so it second one had did a meet the built British car \n",
      "\n",
      "[top 4 token] PROMPT: Cars were invented in\n",
      "Australia in Canada in as not last vehicle horses could not not an an Turbo one \n"
     ]
    }
   ],
   "source": [
    "def print_text(input_tokens, predicted_tensor, top_k=5):\n",
    "    print(f'\\n[top {top_k} token] PROMPT:', line)\n",
    "    for i in range(len(line_indexed) - 1, context_size):\n",
    "        topk = torch.topk(predicted_tensor[0, i, :], top_k)\n",
    "        top_k_predictions = tokenizer.convert_ids_to_tokens(topk.indices)\n",
    "        print(top_k_predictions[top_k - 1], end=' ')\n",
    "    print()\n",
    "        \n",
    "input_text = tokenizer.convert_ids_to_tokens(tokens_tensor.tolist()[0])\n",
    "for i in range(1, 5):\n",
    "    print_text(input_text, predictions, top_k=i)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE**: this text is generated choosing at each step the top_k most probable token.\n",
    "> This is **offline text generation** using the final `prediction` tensor \n",
    "> that has information about the whole sequence (so for each word the prediction has been influenced by the future).\n",
    ">\n",
    "> The text seems worst, probably because the model is trained to optimize the online prediction\n",
    "> like in the previous example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  MODEL INPUTS    MODEL OUTPUT (top 10 tokens)\n",
      "  ------------    -----------------------------\n",
      "* Cars          : . , in and built of to before design (\n",
      "* were          : built the a used made produced in completed first also\n",
      "* invented      : in . , and before at to on from into\n",
      "* in            : the Britain America Australia Canada England France Europe a both\n",
      "  Britain       : . and , in as ; before ( but to\n",
      "  and           : America the American Canada a France Europe Australia North Japan\n",
      "  America       : . , and in before ; as ( but to\n",
      "  ,             : and but so as the although a before which with\n",
      "  but           : the a it not only were in did there had\n",
      "  the           : first only second last third final two fourth largest most\n",
      "  first         : two car one vehicle three was of four product prototype\n",
      "  two           : were cars had horses vehicles of ships in , did\n",
      "  cars          : were had did could , would in needed are came\n",
      "  had           : to been a not problems already the no an had\n",
      "  to            : be have meet not come appear make go receive arrive\n",
      "  have          : been a the an had \" some not were one\n",
      "  been          : a the built an in one Turbo completed \" designed\n",
      "  a             : car \" British Turbo <unk> vehicle two prototype one non\n",
      "  \"             : <unk> Turbo car one two second big \" first B\n"
     ]
    }
   ],
   "source": [
    "def print_input_output(input_tokens, predicted_tensor, top_k=10):\n",
    "    print(f'  MODEL INPUTS    MODEL OUTPUT (top {top_k} tokens)')\n",
    "    print(f'  ------------    -----------------------------')\n",
    "    for i in range(context_size):\n",
    "        topk = torch.topk(predicted_tensor[0, i, :], top_k)\n",
    "        p = '* ' if i < len(line_indexed) else '  '\n",
    "        print(f'{p}{input_tokens[i]:14s}:', end=' ')\n",
    "        top_k_predictions = tokenizer.convert_ids_to_tokens(topk.indices)\n",
    "        print(' '.join(top_k_predictions))\n",
    "        #print('', np.round(topk.values.tolist(), 2))\n",
    "        \n",
    "input_text = tokenizer.convert_ids_to_tokens(tokens_tensor.tolist()[0])\n",
    "print_input_output(input_text, predictions)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE**: lines starting with `*` are inputs in the initial prompt.\n",
    "\n",
    "> **NOTE 2**: the top tokens are imprecise, because the prediction was done online,\n",
    "> while here we use the final `prediction` tensor to score the tokens (offline prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online text generation with sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: Cars were invented in\n",
      "MODEL:  1967 by Dr. Carl R. B. <unk> . In 1968 , the first <unk> were developed by Dr. George P. <unk> and Dr. Charles A. "
     ]
    }
   ],
   "source": [
    "seed = 0\n",
    "prompt = \"Cars were invented in\"\n",
    "max_predictions = 25\n",
    "top_k = 40\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.random.manual_seed(seed)\n",
    "line_tokenized = tokenizer.tokenize(prompt)\n",
    "line_indexed = tokenizer.convert_tokens_to_ids(line_tokenized)\n",
    "tokens_tensor = torch.tensor([line_indexed])\n",
    "tokens_tensor = tokens_tensor.to(device)\n",
    "mems = None\n",
    "\n",
    "print(f'PROMPT: {prompt}')\n",
    "print('MODEL:  ', end='')\n",
    "for i in range(max_predictions):\n",
    "    predictions, mems = model(tokens_tensor, mems=mems)\n",
    "    context_size = tokens_tensor.shape[1]\n",
    "    assert predictions.shape == (1, context_size, vocab_size)\n",
    "    \n",
    "    # sample next token from the most probable top-k\n",
    "    last_prediction = predictions[0, -1, :]\n",
    "    topk = torch.topk(last_prediction, top_k)\n",
    "    log_probs = F.softmax(topk.values, dim=-1)  # softmax among the top-k\n",
    "    rand_idx_in_topk = torch.multinomial(log_probs, num_samples=1)\n",
    "    predicted_index = topk.indices[rand_idx_in_topk]\n",
    "    \n",
    "    # test\n",
    "    last_pred_trunk = top_k_logits(last_prediction.reshape(1, -1), top_k)\n",
    "    sorted_valid_values = last_pred_trunk[last_pred_trunk > -1e10].sort(descending=True).values\n",
    "    assert all(sorted_valid_values == topk.values)\n",
    "    \n",
    "    # update model state\n",
    "    predicted_index = torch.tensor([[predicted_index]]).to(device)\n",
    "    tokens_tensor = torch.cat((tokens_tensor, predicted_index), dim=1)\n",
    "    \n",
    "    # print current token\n",
    "    predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
    "    print(predicted_token, end=' ', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_text_sample(\n",
    "        prompt = \"Cars were invented in\",\n",
    "        seed = 0,\n",
    "        length = 5,\n",
    "        top_k = 40,\n",
    "        top_p = None,\n",
    "    ):\n",
    "    np.random.seed(seed)\n",
    "    torch.random.manual_seed(seed)\n",
    "    line_tokenized = tokenizer.tokenize(prompt)\n",
    "    line_indexed = tokenizer.convert_tokens_to_ids(line_tokenized)\n",
    "    tokens_tensor = torch.tensor([line_indexed])\n",
    "    tokens_tensor = tokens_tensor.to(device)\n",
    "    if top_p is not None:\n",
    "        assert 0 < top_p <= 1, '`top_p` must be in (0..1]'\n",
    "        top_k = round(tokens_tensor.shape[1] * top_p)\n",
    "\n",
    "    print(f'PROMPT: {prompt}')\n",
    "    print('MODEL:  ', end='')\n",
    "    mems = None\n",
    "    for i in range(length):\n",
    "        predictions, mems = model(tokens_tensor, mems=mems)\n",
    "        context_size = tokens_tensor.shape[1]\n",
    "        assert predictions.shape == (1, context_size, vocab_size)\n",
    "\n",
    "        # sample next token from the most probable top-k\n",
    "        last_prediction = predictions[0, -1, :]\n",
    "        topk = torch.topk(last_prediction, top_k)\n",
    "        log_probs = F.softmax(topk.values, dim=-1)  # softmax among the top-k\n",
    "        rand_idx_in_topk = torch.multinomial(log_probs, num_samples=1)\n",
    "        predicted_index = topk.indices[rand_idx_in_topk]\n",
    "\n",
    "        # test\n",
    "        last_pred_trunk = top_k_logits(last_prediction.reshape(1, -1), top_k)\n",
    "        sorted_valid_values = last_pred_trunk[last_pred_trunk > -1e10].sort(descending=True).values\n",
    "        assert all(sorted_valid_values == topk.values)\n",
    "\n",
    "        # update model state\n",
    "        predicted_index = torch.tensor([[predicted_index]]).to(device)\n",
    "        tokens_tensor = torch.cat((tokens_tensor, predicted_index), dim=1)\n",
    "\n",
    "        # print current token\n",
    "        predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
    "        print(predicted_token, end=' ', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: Cars were invented in\n",
      "MODEL:  1967 by Dr. Carl R. B. <unk> . In 1968 "
     ]
    }
   ],
   "source": [
    "prompt = \"Cars were invented in\"\n",
    "gen_text_sample(top_k=40, length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: Cars were invented in\n",
      "MODEL:  Britain and the US . The first cars were produced "
     ]
    }
   ],
   "source": [
    "prompt = \"Cars were invented in\"\n",
    "gen_text_sample(top_p=0.5, length=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: Cars were invented in\n",
      "MODEL:  Britain and "
     ]
    }
   ],
   "source": [
    "prompt = 'What do you know about Machine Learning and Natural Language Processing?'\n",
    "length = 60\n",
    "for seed in range(5):\n",
    "    gen_text_sample(top_p=0.5, length=length, seed=seed)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'What do you know about Machine Learning and Natural Language Processing?'\n",
    "length = 60\n",
    "for seed in range(5):\n",
    "    gen_text_sample(top_k=40, length=length, seed=seed)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file `textgen.py` provides an API for text generation for both *Transformer XL* and other models (*GPT2*, etc..).\n",
    "\n",
    "It requires:\n",
    "\n",
    "- mode signature: `model(prev, past=tensor)` \n",
    "- function `decoder(ids)` returning tokens\n",
    "- from `generate_text_<model>` function, use partial to assign model specific args and create a\n",
    "  function `gen_text` with will have the same signature for all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i textgen.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_comp(prev, past):\n",
    "    return model(prev, mems=past)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = partial(decoder_transformer_xl, tokenizer=tokenizer)\n",
    "#decoder?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_text = partial(generate_text_transformer_xl, model_comp, tokenizer, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context[<class 'list'>]: [17512, 28, 6999, 7]\n",
      "context.shape = torch.Size([1, 4])\n",
      "logits.shape = torch.Size([1, 4, 267735])\n",
      "last_logits.shape = torch.Size([1, 267735])\n",
      "END[topk] logits.shape = torch.Size([1, 4, 267735])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 1/10 [00:03<00:32,  3.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context.shape = torch.Size([1, 5])\n",
      "logits.shape = torch.Size([1, 5, 267735])\n",
      "last_logits.shape = torch.Size([1, 267735])\n",
      "END[topk] logits.shape = torch.Size([1, 5, 267735])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 2/10 [00:07<00:28,  3.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context.shape = torch.Size([1, 6])\n",
      "logits.shape = torch.Size([1, 6, 267735])\n",
      "last_logits.shape = torch.Size([1, 267735])\n",
      "END[topk] logits.shape = torch.Size([1, 6, 267735])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 3/10 [00:10<00:24,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context.shape = torch.Size([1, 7])\n",
      "logits.shape = torch.Size([1, 7, 267735])\n",
      "last_logits.shape = torch.Size([1, 267735])\n",
      "END[topk] logits.shape = torch.Size([1, 7, 267735])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 4/10 [00:12<00:19,  3.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context.shape = torch.Size([1, 8])\n",
      "logits.shape = torch.Size([1, 8, 267735])\n",
      "last_logits.shape = torch.Size([1, 267735])\n",
      "END[topk] logits.shape = torch.Size([1, 8, 267735])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 5/10 [00:15<00:14,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context.shape = torch.Size([1, 9])\n",
      "logits.shape = torch.Size([1, 9, 267735])\n",
      "last_logits.shape = torch.Size([1, 267735])\n",
      "END[topk] logits.shape = torch.Size([1, 9, 267735])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 6/10 [00:17<00:11,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context.shape = torch.Size([1, 10])\n",
      "logits.shape = torch.Size([1, 10, 267735])\n",
      "last_logits.shape = torch.Size([1, 267735])\n",
      "END[topk] logits.shape = torch.Size([1, 10, 267735])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 7/10 [00:20<00:08,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context.shape = torch.Size([1, 11])\n",
      "logits.shape = torch.Size([1, 11, 267735])\n",
      "last_logits.shape = torch.Size([1, 267735])\n",
      "END[topk] logits.shape = torch.Size([1, 11, 267735])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 8/10 [00:22<00:05,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context.shape = torch.Size([1, 12])\n",
      "logits.shape = torch.Size([1, 12, 267735])\n",
      "last_logits.shape = torch.Size([1, 267735])\n",
      "END[topk] logits.shape = torch.Size([1, 12, 267735])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 9/10 [00:26<00:02,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context.shape = torch.Size([1, 13])\n",
      "logits.shape = torch.Size([1, 13, 267735])\n",
      "last_logits.shape = torch.Size([1, 267735])\n",
      "END[topk] logits.shape = torch.Size([1, 13, 267735])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 10/10 [00:29<00:00,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: Cars were invented in\n",
      "======================================== SAMPLE 1 ========================================\n",
      "Britain and America , but the first two cars had\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gen_text(line, \n",
    "         length=10, sample=False, top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "#                     datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "#                     level = logging.INFO)\n",
    "# logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_logits(logits, k):\n",
    "    \"\"\"\n",
    "    Masks everything but the k top entries as -infinity (1e10).\n",
    "    Used to mask logits such that e^-infinity -> 0 won't contribute to the\n",
    "    sum of the denominator.\n",
    "    \"\"\"\n",
    "    if k == 0:\n",
    "        return logits\n",
    "    else:\n",
    "        values = torch.topk(logits, k)[0]\n",
    "        batch_mins = values[:, -1].view(-1, 1).expand_as(logits)\n",
    "        return torch.where(logits < batch_mins, torch.ones_like(logits) * -1e10, logits)\n",
    "\n",
    "    \n",
    "def top_p_logits(logits, p):\n",
    "    \"\"\"\n",
    "    Masks everything but the top-p entries as -infinity (1e10).\n",
    "    \n",
    "    Differently from `top_k_logits`, here we we don't take a fixed number\n",
    "    k of elements in `logits`, but a fraction `p`\n",
    "    of elements. These are the elements higher that the `p` percentile.\n",
    "    \n",
    "    Used to mask logits such that e^-infinity -> 0 won't contribute to the\n",
    "    sum of the denominator.\n",
    "    \"\"\"\n",
    "    if p == 1:\n",
    "        return logits\n",
    "    else:\n",
    "        k = round(logits.shape[1] * p)\n",
    "        print(f'top_p = {top_p:.1g}, k = {k}', flush=True)\n",
    "        return top_k_logits(logits, k)\n",
    "\n",
    "    \n",
    "def sample_sequence(model, length, context, batch_size=None, \n",
    "                    temperature=1, top_k=0, top_p=None, device='cuda', sample=True):\n",
    "    context = torch.tensor(context, device=device, dtype=torch.long).unsqueeze(0).repeat(batch_size, 1)\n",
    "    prev = context\n",
    "    output = context\n",
    "    past = None\n",
    "    with torch.no_grad():\n",
    "        for i in trange(length):\n",
    "            logits, past = model(prev, past=past)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            if top_p is None:\n",
    "                logits = top_k_logits(logits, k=top_k)\n",
    "            else:\n",
    "                logits = top_p_logits(logits, p=top_p)\n",
    "            log_probs = F.softmax(logits, dim=-1)\n",
    "            if sample:\n",
    "                prev = torch.multinomial(log_probs, num_samples=1)\n",
    "            else:\n",
    "                _, prev = torch.topk(logits, k=1, dim=-1)\n",
    "            output = torch.cat((output, prev), dim=1)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def top_k_logits(logits, k):\n",
    "    \"\"\"\n",
    "    Masks everything but the k top entries as -infinity (1e10).\n",
    "    Used to mask logits such that e^-infinity -> 0 won't contribute to the\n",
    "    sum of the denominator.\n",
    "    \"\"\"\n",
    "    if k == 0:\n",
    "        return logits\n",
    "    else:\n",
    "        values = torch.topk(logits, k)[0]\n",
    "        batch_mins = values[:, -1].view(-1, 1).expand_as(logits)\n",
    "        return torch.where(logits < batch_mins, torch.ones_like(logits) * -1e10, logits)\n",
    "\n",
    "def sample_sequence(model, length, start_token=None, batch_size=None, \n",
    "                    context=None, temperature=1, top_k=0, device='cuda', sample=True):\n",
    "#     if start_token is None:\n",
    "#         assert context is not None, 'Specify only one between start_token and context!'\n",
    "#         context = torch.tensor(context, device=device, dtype=torch.long).unsqueeze(0).repeat(batch_size, 1)\n",
    "#     else:\n",
    "#         assert context is None, 'Specify only one between start_token and context!'\n",
    "#         context = torch.full((batch_size, 1), start_token, device=device, dtype=torch.long)\n",
    "    context = torch.tensor(context, device=device, dtype=torch.long).unsqueeze(0).repeat(batch_size, 1)\n",
    "    prev = context\n",
    "    output = context\n",
    "    past = None\n",
    "    with torch.no_grad():\n",
    "\n",
    "    prev = context\n",
    "    output = context\n",
    "    past = None\n",
    "    with torch.no_grad():\n",
    "        for i in trange(length):\n",
    "            logits, past = model(prev, past=past)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            if top_p is None:\n",
    "                logits = top_k_logits(logits, k=top_k)\n",
    "            else:\n",
    "                logits = top_p_logits(logits, p=top_p)\n",
    "            log_probs = F.softmax(logits, dim=-1)\n",
    "            if sample:\n",
    "                prev = torch.multinomial(log_probs, num_samples=1)\n",
    "            else:\n",
    "                _, prev = torch.topk(logits, k=1, dim=-1)\n",
    "            output = torch.cat((output, prev), dim=1)\n",
    "            \n",
    "#             predictions, mems = model(tokens_tensor, mems=mems)\n",
    "#             predicted_index = torch.topk(predictions[0, -1, :],5)[1][1].item()\n",
    "#             predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
    "#             print(predicted_token)\n",
    "#             predicted_index = torch.tensor([[predicted_index]]).to(device)\n",
    "#             tokens_tensor = torch.cat((tokens_tensor, predicted_index), dim=1)\n",
    "            \n",
    "            logits, past = model(prev, mems=past)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            _, prev = torch.topk(logits, k=1, dim=-1)\n",
    "#             _, prev = torch.topk(log_probs, k=1, dim=-1)\n",
    "#            logits = top_k_logits(logits, k=top_k)\n",
    "#            log_probs = F.softmax(logits, dim=-1)\n",
    "#             if sample:\n",
    "#                 prev = torch.multinomial(log_probs, num_samples=1)\n",
    "#             else:\n",
    "#                 _, prev = torch.topk(log_probs, k=1, dim=-1)\n",
    "            output = torch.cat((output, prev), dim=1)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_transformer_xl(text, encoder, device):\n",
    "    text_tokenized = encoder.tokenize(text)\n",
    "    text_indexed = encoder.convert_tokens_to_ids(text_tokenized)\n",
    "    text_indexed_tensor = torch.tensor([text_indexed])\n",
    "    text_indexed_tensor = text_indexed_tensor.to(device)\n",
    "    return text_indexed_tensor\n",
    "\n",
    "def run_model(\n",
    "        prompt = None,\n",
    "        batch_size = 1,\n",
    "        nsamples = 1,    \n",
    "        length = -1,\n",
    "        temperature = 1,\n",
    "        top_k = 0,\n",
    "        top_p=None,\n",
    "        sample = True,\n",
    "        seed = 0,\n",
    "        EOT = '<|endoftext|>',\n",
    "    ):\n",
    "    # Arguments checks\n",
    "    assert nsamples % batch_size == 0\n",
    "    assert prompt is not None and len(prompt) > 0\n",
    "    \n",
    "#     if length == -1:\n",
    "#         length = model.config.n_ctx // 2\n",
    "#     elif length > model.config.n_ctx:\n",
    "#         raise ValueError(\"Can't get samples longer than window size: %s\" % model.config.n_ctx)\n",
    "\n",
    "    # Seed the random-number generators\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "        torch.random.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "    \n",
    "    # Encode prompt (str -> tokens -> tensor(vocabulary))\n",
    "    context_tokens = encode_transformer_xl(prompt, tokenizer, device)\n",
    "\n",
    "    # Generate an output text (multiple times if (nsamples / batch_size) > 1)\n",
    "    generated = 0\n",
    "    for _ in range(nsamples // batch_size):\n",
    "        out = sample_sequence(\n",
    "            model=model, length=length,\n",
    "            context=context_tokens,\n",
    "            batch_size=batch_size,\n",
    "            temperature=temperature, top_k=top_k, device=device, sample=sample,\n",
    "        )\n",
    "        print(f'PROMPT: {prompt}')\n",
    "        out = out[:, len(context_tokens):].tolist()\n",
    "        for i in range(batch_size):\n",
    "            generated += 1\n",
    "            text = enc.decode(out[i])\n",
    "            print(\"=\" * 40 + \" SAMPLE \" + str(generated) + \" \" + \"=\" * 40)\n",
    "            end = text.find(EOT)\n",
    "            end = len(text) if end == -1 else end+len(EOT)\n",
    "            print(text[:end])\n",
    "    print(\"=\" * 80)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransfoXLLMHeadModel(\n",
       "  (transformer): TransfoXLModel(\n",
       "    (word_emb): AdaptiveEmbedding(\n",
       "      (emb_layers): ModuleList(\n",
       "        (0): Embedding(20000, 1024)\n",
       "        (1): Embedding(20000, 256)\n",
       "        (2): Embedding(160000, 64)\n",
       "        (3): Embedding(67735, 16)\n",
       "      )\n",
       "      (emb_projs): ParameterList(\n",
       "          (0): Parameter containing: [torch.FloatTensor of size 1024x1024]\n",
       "          (1): Parameter containing: [torch.FloatTensor of size 1024x256]\n",
       "          (2): Parameter containing: [torch.FloatTensor of size 1024x64]\n",
       "          (3): Parameter containing: [torch.FloatTensor of size 1024x16]\n",
       "      )\n",
       "    )\n",
       "    (drop): Dropout(p=0.1)\n",
       "    (layers): ModuleList(\n",
       "      (0): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (1): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (2): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (3): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (4): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (5): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (6): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (7): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (8): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (9): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (10): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (11): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (12): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (13): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (14): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (15): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (16): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (17): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pos_emb): PositionalEmbedding()\n",
       "  )\n",
       "  (crit): ProjectedAdaptiveLogSoftmax(\n",
       "    (out_layers): ModuleList(\n",
       "      (0): Linear(in_features=1024, out_features=20000, bias=True)\n",
       "      (1): Linear(in_features=256, out_features=20000, bias=True)\n",
       "      (2): Linear(in_features=64, out_features=160000, bias=True)\n",
       "      (3): Linear(in_features=16, out_features=67735, bias=True)\n",
       "    )\n",
       "    (out_projs): ParameterList(\n",
       "        (0): Parameter containing: [torch.FloatTensor of size 1024x1024]\n",
       "        (1): Parameter containing: [torch.FloatTensor of size 1024x256]\n",
       "        (2): Parameter containing: [torch.FloatTensor of size 1024x64]\n",
       "        (3): Parameter containing: [torch.FloatTensor of size 1024x16]\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "torch.random.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name_or_path = 'transfo-xl-wt103'\n",
    "tokenizer = TransfoXLTokenizer.from_pretrained(model_name_or_path)\n",
    "model = TransfoXLLMHeadModel.from_pretrained(model_name_or_path)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"adaptive\": true,\n",
       "  \"attn_type\": 0,\n",
       "  \"clamp_len\": 1000,\n",
       "  \"cutoffs\": [\n",
       "    20000,\n",
       "    40000,\n",
       "    200000\n",
       "  ],\n",
       "  \"d_embed\": 1024,\n",
       "  \"d_head\": 64,\n",
       "  \"d_inner\": 4096,\n",
       "  \"d_model\": 1024,\n",
       "  \"div_val\": 4,\n",
       "  \"dropatt\": 0.0,\n",
       "  \"dropout\": 0.1,\n",
       "  \"ext_len\": 0,\n",
       "  \"init\": \"normal\",\n",
       "  \"init_range\": 0.01,\n",
       "  \"init_std\": 0.02,\n",
       "  \"mem_len\": 1600,\n",
       "  \"n_head\": 16,\n",
       "  \"n_layer\": 18,\n",
       "  \"n_token\": 267735,\n",
       "  \"pre_lnorm\": false,\n",
       "  \"proj_init_std\": 0.01,\n",
       "  \"same_length\": true,\n",
       "  \"sample_softmax\": -1,\n",
       "  \"tgt_len\": 128,\n",
       "  \"tie_projs\": [\n",
       "    false,\n",
       "    true,\n",
       "    true,\n",
       "    true\n",
       "  ],\n",
       "  \"tie_weight\": true,\n",
       "  \"untie_r\": true\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/128 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|          | 1/128 [00:02<05:15,  2.49s/it]\u001b[A\n",
      "  2%|▏         | 2/128 [00:04<05:00,  2.38s/it]\u001b[A\n",
      "  2%|▏         | 3/128 [00:06<04:46,  2.30s/it]\u001b[A\n",
      "  3%|▎         | 4/128 [00:08<04:36,  2.23s/it]\u001b[A\n",
      "  4%|▍         | 5/128 [00:10<04:31,  2.21s/it]\u001b[A\n",
      "  5%|▍         | 6/128 [00:13<04:25,  2.18s/it]\u001b[A\n",
      "  5%|▌         | 7/128 [00:15<04:18,  2.14s/it]\u001b[A\n",
      "  6%|▋         | 8/128 [00:17<04:12,  2.11s/it]\u001b[A\n",
      "  7%|▋         | 9/128 [00:19<04:08,  2.08s/it]\u001b[A\n",
      "  8%|▊         | 10/128 [00:21<04:04,  2.07s/it]\u001b[A\n",
      "  9%|▊         | 11/128 [00:23<03:59,  2.05s/it]\u001b[A\n",
      "  9%|▉         | 12/128 [00:25<03:56,  2.04s/it]\u001b[A\n",
      " 10%|█         | 13/128 [00:27<03:54,  2.04s/it]\u001b[A\n",
      " 11%|█         | 14/128 [00:29<03:51,  2.03s/it]\u001b[A\n",
      " 12%|█▏        | 15/128 [00:31<03:48,  2.03s/it]\u001b[A\n",
      " 12%|█▎        | 16/128 [00:33<03:46,  2.02s/it]\u001b[A\n",
      " 13%|█▎        | 17/128 [00:35<03:44,  2.02s/it]\u001b[A\n",
      " 14%|█▍        | 18/128 [00:37<04:03,  2.21s/it]\u001b[A\n",
      " 15%|█▍        | 19/128 [00:40<03:56,  2.17s/it]\u001b[A\n",
      " 16%|█▌        | 20/128 [00:42<03:51,  2.15s/it]\u001b[A\n",
      " 16%|█▋        | 21/128 [00:44<03:46,  2.12s/it]\u001b[A\n",
      " 17%|█▋        | 22/128 [00:46<03:50,  2.17s/it]\u001b[A\n",
      " 18%|█▊        | 23/128 [00:48<03:46,  2.15s/it]\u001b[A\n",
      " 19%|█▉        | 24/128 [00:50<03:40,  2.12s/it]\u001b[A\n",
      " 20%|█▉        | 25/128 [00:52<03:36,  2.10s/it]\u001b[A\n",
      " 20%|██        | 26/128 [00:54<03:32,  2.09s/it]\u001b[A\n",
      " 21%|██        | 27/128 [00:56<03:29,  2.07s/it]\u001b[A\n",
      " 22%|██▏       | 28/128 [00:58<03:25,  2.05s/it]\u001b[A\n",
      " 23%|██▎       | 29/128 [01:00<03:22,  2.05s/it]\u001b[A\n",
      " 23%|██▎       | 30/128 [01:02<03:19,  2.04s/it]\u001b[A\n",
      " 24%|██▍       | 31/128 [01:04<03:16,  2.03s/it]\u001b[A\n",
      " 25%|██▌       | 32/128 [01:06<03:15,  2.04s/it]\u001b[A\n",
      " 26%|██▌       | 33/128 [01:08<03:12,  2.03s/it]\u001b[A\n",
      " 27%|██▋       | 34/128 [01:10<03:10,  2.03s/it]\u001b[A\n",
      " 27%|██▋       | 35/128 [01:12<03:08,  2.02s/it]\u001b[A\n",
      " 28%|██▊       | 36/128 [01:14<03:05,  2.02s/it]\u001b[A\n",
      " 29%|██▉       | 37/128 [01:16<03:03,  2.02s/it]\u001b[A\n",
      " 30%|██▉       | 38/128 [01:19<03:01,  2.02s/it]\u001b[A\n",
      " 30%|███       | 39/128 [01:21<03:00,  2.02s/it]\u001b[A\n",
      " 31%|███▏      | 40/128 [01:23<02:57,  2.02s/it]\u001b[A\n",
      " 32%|███▏      | 41/128 [01:25<02:54,  2.01s/it]\u001b[A\n",
      " 33%|███▎      | 42/128 [01:27<02:53,  2.01s/it]\u001b[A\n",
      " 34%|███▎      | 43/128 [01:29<02:50,  2.01s/it]\u001b[A\n",
      " 34%|███▍      | 44/128 [01:31<02:48,  2.01s/it]\u001b[A\n",
      " 35%|███▌      | 45/128 [01:33<02:46,  2.00s/it]\u001b[A\n",
      " 36%|███▌      | 46/128 [01:35<02:44,  2.01s/it]\u001b[A\n",
      " 37%|███▋      | 47/128 [01:37<02:43,  2.01s/it]\u001b[A\n",
      " 38%|███▊      | 48/128 [01:39<02:40,  2.01s/it]\u001b[A\n",
      " 38%|███▊      | 49/128 [01:41<02:38,  2.01s/it]\u001b[A\n",
      " 39%|███▉      | 50/128 [01:43<02:36,  2.01s/it]\u001b[A\n",
      " 40%|███▉      | 51/128 [01:45<02:34,  2.00s/it]\u001b[A\n",
      " 41%|████      | 52/128 [01:47<02:32,  2.00s/it]\u001b[A\n",
      " 41%|████▏     | 53/128 [01:49<02:32,  2.03s/it]\u001b[A\n",
      " 42%|████▏     | 54/128 [01:51<02:32,  2.06s/it]\u001b[A\n",
      " 43%|████▎     | 55/128 [01:53<02:29,  2.05s/it]\u001b[A\n",
      " 44%|████▍     | 56/128 [01:55<02:27,  2.05s/it]\u001b[A\n",
      " 45%|████▍     | 57/128 [01:57<02:25,  2.05s/it]\u001b[A\n",
      " 45%|████▌     | 58/128 [01:59<02:24,  2.06s/it]\u001b[A\n",
      " 46%|████▌     | 59/128 [02:01<02:20,  2.04s/it]\u001b[A\n",
      " 47%|████▋     | 60/128 [02:03<02:18,  2.03s/it]\u001b[A\n",
      " 48%|████▊     | 61/128 [02:05<02:15,  2.03s/it]\u001b[A\n",
      " 48%|████▊     | 62/128 [02:07<02:13,  2.02s/it]\u001b[A\n",
      " 49%|████▉     | 63/128 [02:09<02:11,  2.02s/it]\u001b[A\n",
      " 50%|█████     | 64/128 [02:11<02:09,  2.03s/it]\u001b[A\n",
      " 51%|█████     | 65/128 [02:13<02:07,  2.02s/it]\u001b[A\n",
      " 52%|█████▏    | 66/128 [02:15<02:04,  2.02s/it]\u001b[A\n",
      " 52%|█████▏    | 67/128 [02:17<02:02,  2.01s/it]\u001b[A\n",
      " 53%|█████▎    | 68/128 [02:19<02:00,  2.00s/it]\u001b[A\n",
      " 54%|█████▍    | 69/128 [02:21<01:58,  2.01s/it]\u001b[A\n",
      " 55%|█████▍    | 70/128 [02:23<01:56,  2.00s/it]\u001b[A\n",
      " 55%|█████▌    | 71/128 [02:25<01:54,  2.00s/it]\u001b[A\n",
      " 56%|█████▋    | 72/128 [02:27<01:52,  2.00s/it]\u001b[A\n",
      " 57%|█████▋    | 73/128 [02:29<01:50,  2.00s/it]\u001b[A\n",
      " 58%|█████▊    | 74/128 [02:32<02:04,  2.31s/it]\u001b[A\n",
      " 59%|█████▊    | 75/128 [02:37<02:39,  3.02s/it]\u001b[A\n",
      " 59%|█████▉    | 76/128 [02:43<03:22,  3.90s/it]\u001b[A\n",
      " 60%|██████    | 77/128 [02:47<03:15,  3.84s/it]\u001b[A\n",
      " 61%|██████    | 78/128 [02:49<02:49,  3.39s/it]\u001b[A\n",
      " 62%|██████▏   | 79/128 [02:51<02:27,  3.00s/it]\u001b[A\n",
      " 62%|██████▎   | 80/128 [02:53<02:10,  2.72s/it]\u001b[A\n",
      " 63%|██████▎   | 81/128 [02:55<01:58,  2.52s/it]\u001b[A\n",
      " 64%|██████▍   | 82/128 [02:57<01:49,  2.37s/it]\u001b[A\n",
      " 65%|██████▍   | 83/128 [02:59<01:43,  2.31s/it]\u001b[A\n",
      " 66%|██████▌   | 84/128 [03:01<01:38,  2.24s/it]\u001b[A\n",
      " 66%|██████▋   | 85/128 [03:03<01:33,  2.18s/it]\u001b[A\n",
      " 67%|██████▋   | 86/128 [03:05<01:30,  2.15s/it]\u001b[A\n",
      " 68%|██████▊   | 87/128 [03:07<01:26,  2.12s/it]\u001b[A\n",
      " 69%|██████▉   | 88/128 [03:09<01:23,  2.08s/it]\u001b[A\n",
      " 70%|██████▉   | 89/128 [03:11<01:20,  2.05s/it]\u001b[A\n",
      " 70%|███████   | 90/128 [03:13<01:17,  2.04s/it]\u001b[A\n",
      " 71%|███████   | 91/128 [03:15<01:15,  2.03s/it]\u001b[A\n",
      " 72%|███████▏  | 92/128 [03:17<01:12,  2.02s/it]\u001b[A\n",
      " 73%|███████▎  | 93/128 [03:19<01:10,  2.01s/it]\u001b[A\n",
      " 73%|███████▎  | 94/128 [03:21<01:08,  2.00s/it]\u001b[A\n",
      " 74%|███████▍  | 95/128 [03:23<01:06,  2.00s/it]\u001b[A\n",
      " 75%|███████▌  | 96/128 [03:27<01:15,  2.37s/it]\u001b[A\n",
      " 76%|███████▌  | 97/128 [03:29<01:15,  2.45s/it]\u001b[A\n",
      " 77%|███████▋  | 98/128 [03:31<01:09,  2.33s/it]\u001b[A\n",
      " 77%|███████▋  | 99/128 [03:33<01:05,  2.25s/it]\u001b[A\n",
      " 78%|███████▊  | 100/128 [03:36<01:01,  2.20s/it]\u001b[A\n",
      " 79%|███████▉  | 101/128 [03:38<00:58,  2.15s/it]\u001b[A\n",
      " 80%|███████▉  | 102/128 [03:40<00:54,  2.11s/it]\u001b[A\n",
      " 80%|████████  | 103/128 [03:42<00:52,  2.09s/it]\u001b[A\n",
      " 81%|████████▏ | 104/128 [03:44<00:50,  2.08s/it]\u001b[A\n",
      " 82%|████████▏ | 105/128 [03:46<00:47,  2.08s/it]\u001b[A\n",
      " 83%|████████▎ | 106/128 [03:48<00:45,  2.07s/it]\u001b[A\n",
      " 84%|████████▎ | 107/128 [03:50<00:44,  2.11s/it]\u001b[A\n",
      " 84%|████████▍ | 108/128 [03:52<00:42,  2.11s/it]\u001b[A\n",
      " 85%|████████▌ | 109/128 [03:54<00:40,  2.14s/it]\u001b[A\n",
      " 86%|████████▌ | 110/128 [03:57<00:41,  2.29s/it]\u001b[A\n",
      " 87%|████████▋ | 111/128 [03:59<00:37,  2.22s/it]\u001b[A\n",
      " 88%|████████▊ | 112/128 [04:01<00:34,  2.17s/it]\u001b[A\n",
      " 88%|████████▊ | 113/128 [04:03<00:32,  2.14s/it]\u001b[A\n",
      " 89%|████████▉ | 114/128 [04:05<00:29,  2.12s/it]\u001b[A\n",
      " 90%|████████▉ | 115/128 [04:07<00:27,  2.09s/it]\u001b[A\n",
      " 91%|█████████ | 116/128 [04:09<00:24,  2.08s/it]\u001b[A\n",
      " 91%|█████████▏| 117/128 [04:11<00:22,  2.07s/it]\u001b[A\n",
      " 92%|█████████▏| 118/128 [04:13<00:20,  2.08s/it]\u001b[A\n",
      " 93%|█████████▎| 119/128 [04:16<00:18,  2.08s/it]\u001b[A\n",
      " 94%|█████████▍| 120/128 [04:18<00:16,  2.07s/it]\u001b[A\n",
      " 95%|█████████▍| 121/128 [04:20<00:14,  2.06s/it]\u001b[A\n",
      " 95%|█████████▌| 122/128 [04:22<00:12,  2.05s/it]\u001b[A\n",
      " 96%|█████████▌| 123/128 [04:24<00:10,  2.05s/it]\u001b[A\n",
      " 97%|█████████▋| 124/128 [04:29<00:11,  2.90s/it]\u001b[A\n",
      " 98%|█████████▊| 125/128 [04:32<00:08,  2.93s/it]\u001b[A\n",
      " 98%|█████████▊| 126/128 [04:34<00:05,  2.67s/it]\u001b[A\n",
      " 99%|█████████▉| 127/128 [04:36<00:02,  2.49s/it]\u001b[A\n",
      "100%|██████████| 128/128 [04:38<00:00,  2.36s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================== SAMPLE 1 ========================================\n",
      "do you know about Machine Learning and Natural Language <unk> , <eos> <eos> = = = = <unk> = = = = <eos> <eos> The first recorded instance of a <unk> was in the early 20th century , when a man named <unk> was arrested for stealing a horse from a horse . He was charged with theft and sentenced to six months in prison . <eos> <eos> = = = = <unk> = = = = <eos> <eos> The first recorded instance of a <unk> was in the early 20th century , when a man named <unk> was arrested for stealing a horse from a horse . He was sentenced to six months in prison and fined Â£ 1 @,@ 000 . <eos> <eos> = = = = <unk> = = = = <eos> <eos> The first\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "torch.random.manual_seed(seed)\n",
    "run_model('What do you know about Machine Learning and Natural Language Processing?', length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:09<00:00, 13.55it/s]\n",
      "  0%|          | 0/128 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================== SAMPLE 1 ========================================\n",
      " Isn't it remarkable how powerful it is to let your mind run free of everything that's changing around it?\n",
      "\n",
      "We've worked at Machine Learning Hub for more than 20 years and have discovered that as a beginning along the way we are primed for processes that are indistinguishable from human behavior.\n",
      "\n",
      "Our first example premiered at the exact moment in history when Norma Bolt, a templated data scientist with the former Company That Builds Everything, won first prize to shut her down. One year later Bolt decided to give Carlos Miller fiscal incentives to write an article on Machine Learning and left working on it (besides the Ask Me Anything\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:10<00:00, 12.38it/s]\n",
      "  1%|          | 1/128 [00:00<00:16,  7.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================== SAMPLE 1 ========================================\n",
      "\n",
      "\n",
      "If you're a teacher or teacher person and you take the time to learn more about Machine Learning Prediction Python written by three people smoking cigarettes: VThisuppolo Stefano aka 'teacher' and Asundri 'magic' Alibato\n",
      "\n",
      "Published – May 22, 2018\n",
      "\n",
      "PETROSOFT THUNDER\n",
      "\n",
      "Artistic Attorney is one of the recent founders\n",
      "\n",
      "developed by Pierre Tonkin Architects for the Jesuit university of Sepho on the outskirts of Groningen. The 42 year old TorboÍso Pascal reflects particularly on the process behind this project:\n",
      "\n",
      "Pierre has been true to his\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:09<00:00, 12.84it/s]\n",
      "  1%|          | 1/128 [00:00<00:17,  7.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================== SAMPLE 1 ========================================\n",
      " Student: Machine Learning Designers. Tarryl: Machine Learning Tools for Education Programs. Roger: Virtual Reality Knowledge Base.\n",
      "\n",
      "4. CompuServe\n",
      "\n",
      "Advantages:\n",
      "\n",
      "- Awesome movies in online teacher education reviews.\n",
      "\n",
      "- Much of their online sales are focused around student reviews.\n",
      "\n",
      "- Page maintenance tools in user interfaces. Easy online assessment tool.\n",
      "\n",
      "- Augmented class discussions for students\n",
      "\n",
      "- Quality product reviews and maintainers that are extremely professional.\n",
      "\n",
      "- Lots of pmsc or categories for students.\n",
      "\n",
      "Many possibilities, for user becomes its own question particle patterns\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:08<00:00, 14.74it/s]\n",
      "  1%|          | 1/128 [00:00<00:15,  8.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================== SAMPLE 1 ========================================\n",
      "\n",
      "\n",
      "Machine learning doesn't just mean running checks to learn new evidence based on an experiment run. You also have to build a model that knows how much to be different. Machine learning can work inside your data and attack your data over time.\n",
      "\n",
      "So how do you choose the right tool that you need, and what tools discourage you from using?\n",
      "\n",
      "Learning and Training Is a 2nd Step to Thinking Pavlov's Big Ideas\n",
      "\n",
      "Here's a short summary of what's being talked about on Machine Learning: you can read a lot more about Machine Learning before you sign up for DevOps or MySQL or Bootstrap business\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:08<00:00, 15.71it/s]\n",
      "  1%|          | 1/128 [00:00<00:14,  8.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================== SAMPLE 1 ========================================\n",
      "\n",
      "\n",
      "\n",
      "I bought MariaScript for Linux. It is extremely fast! It was ever using file system system when I was running under Windows, Python and other code in Windows, so oh yeah, here we go. It is not all human-readable PCI-Express, but it works and is pretty secure, it works as many Unix or Windows machines as it can, and I think it does it all pretty nice! There are numerous things Python hasn't yet learned, but I suspect it will provide has probably at least one of them. Probably around 16 or more frameworks that Alexandros Berskov will use in open source or he'll become\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:09<00:00, 14.54it/s]\n",
      "  1%|          | 1/128 [00:00<00:13,  9.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================== SAMPLE 1 ========================================\n",
      " How do you use it in your applications? How do you enable it? 0 of 10 Next Get answers in a Quora question.\n",
      "\n",
      "Natural Language Processing is the Processing of Data from One Medium by Jason Grossier\n",
      "\n",
      "Daitsy writes: \"The story of how artificial intelligence has come up with a very convincing model that predicts very heavily on our history: the path of a typical human home.\" That's an interesting line in the wrong direction. Only in 2017 have computer scientists quite managed to figure it out.\n",
      "\n",
      "The year 2001 was the 'golden window' was set. Robin Kamms was the CIA director\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:09<00:00, 14.16it/s]\n",
      "  1%|          | 1/128 [00:00<00:14,  8.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================== SAMPLE 1 ========================================\n",
      " You probably heard from many companies. Here are five that I realize are leading Silicon Valley companies that are leading these industries.\n",
      "\n",
      "Step 1 – DevOps\n",
      "\n",
      "Almost everyone I talk to who's considering jumping into the tech industry has stated their intent to start a take on DevOps. Creating and using Container Containerization for the IT industry is the most obvious example of CI — DevOps isn't about tracking down required employee iterations (\".asp file info\"File'dctlHelp\", etc.) or cluttering up regular deploys to EOM's. But creating quick deployment pipelines for Docker containers is however the name. In your approach,\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:08<00:00, 14.83it/s]\n",
      "  1%|          | 1/128 [00:00<00:14,  8.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================== SAMPLE 1 ========================================\n",
      "\n",
      "\n",
      "Machine Learning engineers simply use Translate which is a data-rich algorithm, crafted to automate mathematical verification and store tens of billions of data streams in a secure database.\n",
      "\n",
      "YOUR GLOBAL LEADER\n",
      "\n",
      "How did your time at University Connect image manipulation workshop effort impact your skill set?\n",
      "\n",
      "At Baccalaureate I found that we prioritized knowledge over experience & mentorship tenure, leadership, actual work, and keep it real. In addition to our many ancestors, our knowledgeable Chinese and Vietnamese colleagues learn through field education & talent exchange—contorically crafts that empower us to be as trustworthy as we\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:09<00:00, 13.83it/s]\n",
      "  1%|          | 1/128 [00:00<00:16,  7.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================== SAMPLE 1 ========================================\n",
      " Let me know in the comments below!\n",
      "\n",
      "Image credits: Wikimedia Commons , dboven, GPSImagesMagnet.com, iLibrary http://tsk<|endoftext|>NEW DELHI: India is facing a serious crisis over what can happen when it needs to cull 2,500 trainees once it stalks express train with a massive explosive discovered here late last month.\n",
      "\n",
      "Experts say constraints imposed on trainmen by railway authorities and lawmakers have allowed the project to fail, leading to an \"inability\" to get train utilised in the next couple of weeks.\n",
      "\n",
      "Officials point out that intrusive screening of element 500 used in Pakistan\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:08<00:00, 15.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================== SAMPLE 1 ========================================\n",
      "\n",
      "\n",
      "Allen: Well, I first wrote about understanding machine learning after a really cold RSS feed that I've seen to the nearest 100,000 on Slack. Sort of back to hacker stuff. I've learned a lot about language driven AI through my experiences with AI modelling, how the big pin backpacks really work. Also, understanding the bigger problems is difficult but worth it so far. If you can print an entirely pre-programmed neural model and it turns out you can do pre-code which is interesting to know what you're doing with that sensor on your TV that the AI is chasing for.\n",
      "\n",
      "And so claiming\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "seed = 0\n",
    "for seed in range(10):\n",
    "    np.random.seed(seed)\n",
    "    torch.random.manual_seed(seed)\n",
    "    run_model('What do you know about Machine Learning and Natural Language Processing?', length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (fastai)",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
