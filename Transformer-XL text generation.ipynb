{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from tqdm import trange\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pytorch_pretrained_bert\n",
    "from pytorch_pretrained_bert import TransfoXLTokenizer, TransfoXLModel, TransfoXLLMHeadModel\n",
    "for mod in (np, torch, pytorch_pretrained_bert):\n",
    "    print(f'{mod.__name__}: {mod.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model Transformer XL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "torch.random.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name_or_path = 'transfo-xl-wt103'\n",
    "tokenizer = TransfoXLTokenizer.from_pretrained(model_name_or_path)\n",
    "model = TransfoXLLMHeadModel.from_pretrained(model_name_or_path)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dummy prediction, to check vocab size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = \"Dummy\"\n",
    "line_tokenized = tokenizer.tokenize(line)\n",
    "line_indexed = tokenizer.convert_tokens_to_ids(line_tokenized)\n",
    "tokens_tensor = torch.tensor([line_indexed])\n",
    "predictions, _ = model(tokens_tensor)\n",
    "vocab_size = predictions.shape[-1]\n",
    "assert vocab_size == 267735  # WikiText-103 vocab size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimal example\n",
    "\n",
    "## Online text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = \"Cars were invented in\"\n",
    "max_predictions = 16\n",
    "top_k = 2\n",
    "\n",
    "line_tokenized = tokenizer.tokenize(line)\n",
    "line_indexed = tokenizer.convert_tokens_to_ids(line_tokenized)\n",
    "tokens_tensor = torch.tensor([line_indexed])\n",
    "tokens_tensor = tokens_tensor.to(device)\n",
    "mems = None\n",
    "\n",
    "for i in range(max_predictions):\n",
    "    predictions, mems = model(tokens_tensor, mems=mems)\n",
    "    context_size = tokens_tensor.shape[1]\n",
    "    assert predictions.shape == (1, context_size, vocab_size)\n",
    "    topk = torch.topk(predictions[0, -1, :], 10)\n",
    "    predicted_index = topk.indices[top_k-1].item()\n",
    "    predicted_index = torch.tensor([[predicted_index]]).to(device)\n",
    "    tokens_tensor = torch.cat((tokens_tensor, predicted_index), dim=1)\n",
    "    \n",
    "    predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
    "    print(predicted_token, end=' ', flush=True)\n",
    "    print('\\n', tokenizer.convert_ids_to_tokens(topk.indices), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE**: this text is generated choosing at each step the top_k most probable token.\n",
    "> This is **online text generation**, since at each step, the model only knows the past."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Off-line text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_text(input_tokens, predicted_tensor, top_k=5):\n",
    "    print(f'\\n[top {top_k} token] PROMPT:', line)\n",
    "    for i in range(len(line_indexed) - 1, context_size):\n",
    "        topk = torch.topk(predicted_tensor[0, i, :], top_k)\n",
    "        top_k_predictions = tokenizer.convert_ids_to_tokens(topk.indices)\n",
    "        print(top_k_predictions[top_k - 1], end=' ')\n",
    "    print()\n",
    "        \n",
    "input_text = tokenizer.convert_ids_to_tokens(tokens_tensor.tolist()[0])\n",
    "for i in range(1, 5):\n",
    "    print_text(input_text, predictions, top_k=i)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE**: this text is generated choosing at each step the top_k most probable token.\n",
    "> This is **offline text generation** using the final `prediction` tensor \n",
    "> that has information about the whole sequence (so for each word the prediction has been influenced by the future).\n",
    ">\n",
    "> The text seems worst, probably because the model is trained to optimize the online prediction\n",
    "> like in the previous example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_input_output(input_tokens, predicted_tensor, top_k=10):\n",
    "    print(f'  MODEL INPUTS    MODEL OUTPUT (top {top_k} tokens)')\n",
    "    print(f'  ------------    -----------------------------')\n",
    "    for i in range(context_size):\n",
    "        topk = torch.topk(predicted_tensor[0, i, :], top_k)\n",
    "        p = '* ' if i < len(line_indexed) else '  '\n",
    "        print(f'{p}{input_tokens[i]:14s}:', end=' ')\n",
    "        top_k_predictions = tokenizer.convert_ids_to_tokens(topk.indices)\n",
    "        print(' '.join(top_k_predictions))\n",
    "        #print('', np.round(topk.values.tolist(), 2))\n",
    "        \n",
    "input_text = tokenizer.convert_ids_to_tokens(tokens_tensor.tolist()[0])\n",
    "print_input_output(input_text, predictions)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE**: lines starting with `*` are inputs in the initial prompt.\n",
    "\n",
    "> **NOTE 2**: the top tokens are imprecise, because the prediction was done online,\n",
    "> while here we use the final `prediction` tensor to score the tokens (offline prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online text generation with sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "prompt = \"Cars were invented in\"\n",
    "max_predictions = 25\n",
    "top_k = 40\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.random.manual_seed(seed)\n",
    "line_tokenized = tokenizer.tokenize(prompt)\n",
    "line_indexed = tokenizer.convert_tokens_to_ids(line_tokenized)\n",
    "tokens_tensor = torch.tensor([line_indexed])\n",
    "tokens_tensor = tokens_tensor.to(device)\n",
    "mems = None\n",
    "\n",
    "print(f'PROMPT: {prompt}')\n",
    "print('MODEL:  ', end='')\n",
    "for i in range(max_predictions):\n",
    "    predictions, mems = model(tokens_tensor, mems=mems)\n",
    "    context_size = tokens_tensor.shape[1]\n",
    "    assert predictions.shape == (1, context_size, vocab_size)\n",
    "    \n",
    "    # sample next token from the most probable top-k\n",
    "    last_prediction = predictions[0, -1, :]\n",
    "    topk = torch.topk(last_prediction, top_k)\n",
    "    log_probs = F.softmax(topk.values, dim=-1)  # softmax among the top-k\n",
    "    rand_idx_in_topk = torch.multinomial(log_probs, num_samples=1)\n",
    "    predicted_index = topk.indices[rand_idx_in_topk]\n",
    "    \n",
    "    # test\n",
    "    last_pred_trunk = top_k_logits(last_prediction.reshape(1, -1), top_k)\n",
    "    sorted_valid_values = last_pred_trunk[last_pred_trunk > -1e10].sort(descending=True).values\n",
    "    assert all(sorted_valid_values == topk.values)\n",
    "    \n",
    "    # update model state\n",
    "    predicted_index = torch.tensor([[predicted_index]]).to(device)\n",
    "    tokens_tensor = torch.cat((tokens_tensor, predicted_index), dim=1)\n",
    "    \n",
    "    # print current token\n",
    "    predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
    "    print(predicted_token, end=' ', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_text_sample(\n",
    "        prompt = \"Cars were invented in\",\n",
    "        seed = 0,\n",
    "        length = 5,\n",
    "        top_k = 40,\n",
    "        top_p = None,\n",
    "    ):\n",
    "    np.random.seed(seed)\n",
    "    torch.random.manual_seed(seed)\n",
    "    line_tokenized = tokenizer.tokenize(prompt)\n",
    "    line_indexed = tokenizer.convert_tokens_to_ids(line_tokenized)\n",
    "    tokens_tensor = torch.tensor([line_indexed])\n",
    "    tokens_tensor = tokens_tensor.to(device)\n",
    "    if top_p is not None:\n",
    "        assert 0 < top_p <= 1, '`top_p` must be in (0..1]'\n",
    "        top_k = round(tokens_tensor.shape[1] * top_p)\n",
    "\n",
    "    print(f'PROMPT: {prompt}')\n",
    "    print('MODEL:  ', end='')\n",
    "    mems = None\n",
    "    for i in range(length):\n",
    "        predictions, mems = model(tokens_tensor, mems=mems)\n",
    "        context_size = tokens_tensor.shape[1]\n",
    "        assert predictions.shape == (1, context_size, vocab_size)\n",
    "\n",
    "        # sample next token from the most probable top-k\n",
    "        last_prediction = predictions[0, -1, :]\n",
    "        topk = torch.topk(last_prediction, top_k)\n",
    "        log_probs = F.softmax(topk.values, dim=-1)  # softmax among the top-k\n",
    "        rand_idx_in_topk = torch.multinomial(log_probs, num_samples=1)\n",
    "        predicted_index = topk.indices[rand_idx_in_topk]\n",
    "\n",
    "        # test\n",
    "        last_pred_trunk = top_k_logits(last_prediction.reshape(1, -1), top_k)\n",
    "        sorted_valid_values = last_pred_trunk[last_pred_trunk > -1e10].sort(descending=True).values\n",
    "        assert all(sorted_valid_values == topk.values)\n",
    "\n",
    "        # update model state\n",
    "        predicted_index = torch.tensor([[predicted_index]]).to(device)\n",
    "        tokens_tensor = torch.cat((tokens_tensor, predicted_index), dim=1)\n",
    "\n",
    "        # print current token\n",
    "        predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
    "        print(predicted_token, end=' ', flush=True)\n",
    "        \n",
    "        if predicted_token == '=':\n",
    "            break\n",
    "            # avoid wasting time with bad predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Cars were invented in\"\n",
    "gen_text_sample(top_k=40, length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Cars were invented in\"\n",
    "gen_text_sample(top_p=0.5, length=10)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "length = 60\n",
    "for seed in range(5):\n",
    "    gen_text_sample(top_p=0.9, length=length, seed=seed)\n",
    "    print('\\n')\n",
    "\n",
    "\n",
    "PROMPT: Cars were invented in\n",
    "MODEL:  1978 , and were built into cars in the 1980s . <eos> <eos> = \n",
    "\n",
    "PROMPT: Cars were invented in\n",
    "MODEL:  the late 1950s . <eos> <eos> = \n",
    "\n",
    "PROMPT: Cars were invented in\n",
    "MODEL:  late 1950s cars . In the early 1960s cars were built in many different styles . The first car to use the V12 engine was the hatchback , which was introduced in late 1957 . <eos> <eos> = \n",
    "\n",
    "PROMPT: Cars were invented in\n",
    "MODEL:  the 1930s , and the <unk> were built in the 1930s . <eos> <eos> = \n",
    "\n",
    "PROMPT: Cars were invented in\n",
    "MODEL:  the 1970s . In 1982 , the cars were invented in the 1980s in the Cars . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: What do you know about Machine Learning and Natural Language Processing?\n",
      "MODEL:  for a \" real <unk> . \" <eos> What do you know about Machine Learning and Natural Language <unk> for a \" <unk> <unk> . \" <eos> What do you know about Machine Learning and natural Language <unk> for a \" <unk> <unk> . \" <eos> What do I know about Machine Learning , Natural Language <unk> for a \" \n",
      "\n",
      "PROMPT: What do you know about Machine Learning and Natural Language Processing?\n",
      "MODEL:  , <eos> <eos> = \n",
      "\n",
      "PROMPT: What do you know about Machine Learning and Natural Language Processing?\n",
      "MODEL:  is <unk> , a book about the <unk> , which is about the <unk> of <unk> ( <unk> , <unk> ) ) and is <unk> , a <unk> , and <unk> , a <unk> <unk> , <unk> on the <unk> of <unk> and is <unk> . <unk> , an <unk> <unk> , is <unk> , a <unk> <unk> , <unk> \n",
      "\n",
      "PROMPT: What do you know about Machine Learning and Natural Language Processing?\n",
      "MODEL:  , <eos> the only way to know about Machine Learning and natural Language <unk> and the way to learn about Machine Learning and the way to understand the <unk> , <eos> the only way to understand the language <unk> . <eos> <eos> = \n",
      "\n",
      "PROMPT: What do you know about Machine Learning and Natural Language Processing?\n",
      "MODEL:  . \" <eos> As described above , Machine Learning and Natural Language <unk> <unk> <unk> . \" <eos> As described above , Machine Learning and Natural Language <unk> . \" <eos> As described above , Machine Learning and Natural Language <unk> . \" As described above , Machine Learning and Fine Language <unk> <unk> <unk> . <eos> As described above \n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = 'What do you know about Machine Learning and Natural Language Processing?'\n",
    "length = 60\n",
    "for seed in range(5):\n",
    "    gen_text_sample(prompt, top_p=0.9, length=length, seed=seed)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: What do you know about Machine Learning and Natural Language Processing?\n",
      "MODEL:  has and is part of . <eos> The most important part of the book is about the development of a new theory of Machine Learning and how it could be integrated into the new theory that was being implemented . <eos> <eos> = \n",
      "\n",
      "PROMPT: What do you know about Machine Learning and Natural Language Processing?\n",
      "MODEL:  , <eos> <eos> = \n",
      "\n",
      "PROMPT: What do you know about Machine Learning and Natural Language Processing?\n",
      "MODEL:  @-@ <unk> , ? â ? <eos> The questions about the ways a computer could be of use with the work that I have about the job of developing the language <unk> @-@ <unk> , ? ? â ? <eos> What do you know about a computer ? â ? â ? â ? â ? â ? â ? \n",
      "\n",
      "PROMPT: What do you know about Machine Learning and Natural Language Processing?\n",
      "MODEL:  , <eos> a system for teaching the lessons of Natural Language <unk> , <eos> which is a system used "
     ]
    }
   ],
   "source": [
    "prompt = 'What do you know about Machine Learning and Natural Language Processing?<eos> Natural Language Processing is a branche of machine learning'\n",
    "length = 60\n",
    "for seed in range(5):\n",
    "    gen_text_sample(prompt, top_k=40, length=length, seed=seed)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: What do you know about Machine Learning and Natural Language Processing?\n",
      "MODEL:  has and is part of . <eos> The most important part of the book is about the development of a new theory of Machine Learning and how it could be integrated into the new theory that was being implemented . <eos> <eos> = \n",
      "\n",
      "PROMPT: What do you know about Machine Learning and Natural Language Processing?\n",
      "MODEL:  , <eos> <eos> = \n",
      "\n",
      "PROMPT: What do you know about Machine Learning and Natural Language Processing?\n",
      "MODEL:  @-@ <unk> , ? â ? <eos> The questions about the ways a computer could be of use with the work that I have about the job of developing the language <unk> @-@ <unk> , ? ? â ? <eos> What do you know about a computer ? â ? â ? â ? â ? â ? â ? \n",
      "\n",
      "PROMPT: What do you know about Machine Learning and Natural Language Processing?\n",
      "MODEL:  , <eos> a system for teaching the lessons of Natural Language <unk> , <eos> "
     ]
    }
   ],
   "source": [
    "prompt = ('What do you know about Machine Learning and Natural Language Processing?<eos>\n",
    "          'Natural Language Processing is a branche of machine learning ')\n",
    "length = 60\n",
    "for seed in range(5):\n",
    "    gen_text_sample(prompt, top_k=40, length=length, seed=seed)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file `textgen.py` provides an API for text generation for both *Transformer XL* and other models (*GPT2*, etc..).\n",
    "\n",
    "It requires:\n",
    "\n",
    "- mode signature: `model(prev, past=tensor)` \n",
    "- function `decoder(ids)` returning tokens\n",
    "- from `generate_text_<model>` function, use partial to assign model specific args and create a\n",
    "  function `gen_text` with will have the same signature for all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i textgen.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_comp(prev, past):\n",
    "    return model(prev, mems=past)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = partial(decoder_transformer_xl, tokenizer=tokenizer)\n",
    "#decoder?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_text = partial(generate_text_transformer_xl, model_comp, tokenizer, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_text(line, \n",
    "         length=10, sample=False, top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "#                     datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "#                     level = logging.INFO)\n",
    "# logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_logits(logits, k):\n",
    "    \"\"\"\n",
    "    Masks everything but the k top entries as -infinity (1e10).\n",
    "    Used to mask logits such that e^-infinity -> 0 won't contribute to the\n",
    "    sum of the denominator.\n",
    "    \"\"\"\n",
    "    if k == 0:\n",
    "        return logits\n",
    "    else:\n",
    "        values = torch.topk(logits, k)[0]\n",
    "        batch_mins = values[:, -1].view(-1, 1).expand_as(logits)\n",
    "        return torch.where(logits < batch_mins, torch.ones_like(logits) * -1e10, logits)\n",
    "\n",
    "    \n",
    "def top_p_logits(logits, p):\n",
    "    \"\"\"\n",
    "    Masks everything but the top-p entries as -infinity (1e10).\n",
    "    \n",
    "    Differently from `top_k_logits`, here we we don't take a fixed number\n",
    "    k of elements in `logits`, but a fraction `p`\n",
    "    of elements. These are the elements higher that the `p` percentile.\n",
    "    \n",
    "    Used to mask logits such that e^-infinity -> 0 won't contribute to the\n",
    "    sum of the denominator.\n",
    "    \"\"\"\n",
    "    if p == 1:\n",
    "        return logits\n",
    "    else:\n",
    "        k = round(logits.shape[1] * p)\n",
    "        print(f'top_p = {top_p:.1g}, k = {k}', flush=True)\n",
    "        return top_k_logits(logits, k)\n",
    "\n",
    "    \n",
    "def sample_sequence(model, length, context, batch_size=None, \n",
    "                    temperature=1, top_k=0, top_p=None, device='cuda', sample=True):\n",
    "    context = torch.tensor(context, device=device, dtype=torch.long).unsqueeze(0).repeat(batch_size, 1)\n",
    "    prev = context\n",
    "    output = context\n",
    "    past = None\n",
    "    with torch.no_grad():\n",
    "        for i in trange(length):\n",
    "            logits, past = model(prev, past=past)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            if top_p is None:\n",
    "                logits = top_k_logits(logits, k=top_k)\n",
    "            else:\n",
    "                logits = top_p_logits(logits, p=top_p)\n",
    "            log_probs = F.softmax(logits, dim=-1)\n",
    "            if sample:\n",
    "                prev = torch.multinomial(log_probs, num_samples=1)\n",
    "            else:\n",
    "                _, prev = torch.topk(logits, k=1, dim=-1)\n",
    "            output = torch.cat((output, prev), dim=1)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def top_k_logits(logits, k):\n",
    "    \"\"\"\n",
    "    Masks everything but the k top entries as -infinity (1e10).\n",
    "    Used to mask logits such that e^-infinity -> 0 won't contribute to the\n",
    "    sum of the denominator.\n",
    "    \"\"\"\n",
    "    if k == 0:\n",
    "        return logits\n",
    "    else:\n",
    "        values = torch.topk(logits, k)[0]\n",
    "        batch_mins = values[:, -1].view(-1, 1).expand_as(logits)\n",
    "        return torch.where(logits < batch_mins, torch.ones_like(logits) * -1e10, logits)\n",
    "\n",
    "def sample_sequence(model, length, start_token=None, batch_size=None, \n",
    "                    context=None, temperature=1, top_k=0, device='cuda', sample=True):\n",
    "#     if start_token is None:\n",
    "#         assert context is not None, 'Specify only one between start_token and context!'\n",
    "#         context = torch.tensor(context, device=device, dtype=torch.long).unsqueeze(0).repeat(batch_size, 1)\n",
    "#     else:\n",
    "#         assert context is None, 'Specify only one between start_token and context!'\n",
    "#         context = torch.full((batch_size, 1), start_token, device=device, dtype=torch.long)\n",
    "    context = torch.tensor(context, device=device, dtype=torch.long).unsqueeze(0).repeat(batch_size, 1)\n",
    "    prev = context\n",
    "    output = context\n",
    "    past = None\n",
    "    with torch.no_grad():\n",
    "\n",
    "    prev = context\n",
    "    output = context\n",
    "    past = None\n",
    "    with torch.no_grad():\n",
    "        for i in trange(length):\n",
    "            logits, past = model(prev, past=past)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            if top_p is None:\n",
    "                logits = top_k_logits(logits, k=top_k)\n",
    "            else:\n",
    "                logits = top_p_logits(logits, p=top_p)\n",
    "            log_probs = F.softmax(logits, dim=-1)\n",
    "            if sample:\n",
    "                prev = torch.multinomial(log_probs, num_samples=1)\n",
    "            else:\n",
    "                _, prev = torch.topk(logits, k=1, dim=-1)\n",
    "            output = torch.cat((output, prev), dim=1)\n",
    "            \n",
    "#             predictions, mems = model(tokens_tensor, mems=mems)\n",
    "#             predicted_index = torch.topk(predictions[0, -1, :],5)[1][1].item()\n",
    "#             predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
    "#             print(predicted_token)\n",
    "#             predicted_index = torch.tensor([[predicted_index]]).to(device)\n",
    "#             tokens_tensor = torch.cat((tokens_tensor, predicted_index), dim=1)\n",
    "            \n",
    "            logits, past = model(prev, mems=past)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            _, prev = torch.topk(logits, k=1, dim=-1)\n",
    "#             _, prev = torch.topk(log_probs, k=1, dim=-1)\n",
    "#            logits = top_k_logits(logits, k=top_k)\n",
    "#            log_probs = F.softmax(logits, dim=-1)\n",
    "#             if sample:\n",
    "#                 prev = torch.multinomial(log_probs, num_samples=1)\n",
    "#             else:\n",
    "#                 _, prev = torch.topk(log_probs, k=1, dim=-1)\n",
    "            output = torch.cat((output, prev), dim=1)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_transformer_xl(text, encoder, device):\n",
    "    text_tokenized = encoder.tokenize(text)\n",
    "    text_indexed = encoder.convert_tokens_to_ids(text_tokenized)\n",
    "    text_indexed_tensor = torch.tensor([text_indexed])\n",
    "    text_indexed_tensor = text_indexed_tensor.to(device)\n",
    "    return text_indexed_tensor\n",
    "\n",
    "def run_model(\n",
    "        prompt = None,\n",
    "        batch_size = 1,\n",
    "        nsamples = 1,    \n",
    "        length = -1,\n",
    "        temperature = 1,\n",
    "        top_k = 0,\n",
    "        top_p=None,\n",
    "        sample = True,\n",
    "        seed = 0,\n",
    "        EOT = '<|endoftext|>',\n",
    "    ):\n",
    "    # Arguments checks\n",
    "    assert nsamples % batch_size == 0\n",
    "    assert prompt is not None and len(prompt) > 0\n",
    "    \n",
    "#     if length == -1:\n",
    "#         length = model.config.n_ctx // 2\n",
    "#     elif length > model.config.n_ctx:\n",
    "#         raise ValueError(\"Can't get samples longer than window size: %s\" % model.config.n_ctx)\n",
    "\n",
    "    # Seed the random-number generators\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "        torch.random.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "    \n",
    "    # Encode prompt (str -> tokens -> tensor(vocabulary))\n",
    "    context_tokens = encode_transformer_xl(prompt, tokenizer, device)\n",
    "\n",
    "    # Generate an output text (multiple times if (nsamples / batch_size) > 1)\n",
    "    generated = 0\n",
    "    for _ in range(nsamples // batch_size):\n",
    "        out = sample_sequence(\n",
    "            model=model, length=length,\n",
    "            context=context_tokens,\n",
    "            batch_size=batch_size,\n",
    "            temperature=temperature, top_k=top_k, device=device, sample=sample,\n",
    "        )\n",
    "        print(f'PROMPT: {prompt}')\n",
    "        out = out[:, len(context_tokens):].tolist()\n",
    "        for i in range(batch_size):\n",
    "            generated += 1\n",
    "            text = enc.decode(out[i])\n",
    "            print(\"=\" * 40 + \" SAMPLE \" + str(generated) + \" \" + \"=\" * 40)\n",
    "            end = text.find(EOT)\n",
    "            end = len(text) if end == -1 else end+len(EOT)\n",
    "            print(text[:end])\n",
    "    print(\"=\" * 80)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "torch.random.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name_or_path = 'transfo-xl-wt103'\n",
    "tokenizer = TransfoXLTokenizer.from_pretrained(model_name_or_path)\n",
    "model = TransfoXLLMHeadModel.from_pretrained(model_name_or_path)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "torch.random.manual_seed(seed)\n",
    "run_model('What do you know about Machine Learning and Natural Language Processing?', length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "seed = 0\n",
    "for seed in range(10):\n",
    "    np.random.seed(seed)\n",
    "    torch.random.manual_seed(seed)\n",
    "    run_model('What do you know about Machine Learning and Natural Language Processing?', length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (fastai)",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
